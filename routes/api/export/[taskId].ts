// routes/api/export/[taskId].ts - ä¿®å¤ç‰ˆä¸‹è½½ç«¯ç‚¹

import { FreshContext, Handlers } from "$fresh/server.ts";
import * as fflate from "fflate";
import { getKv } from "../../../utils/kv.ts";
import { getExtensionFromResponse, formatDateForFilename, sanitizeFilename } from "../../../utils/fileUtils.ts";

interface TaskMeta {
  taskId: string;
  userToken: string; // Store a portion of the user token for identification
  teamId?: string;
  includeMetadata: boolean;
  includeThumbnails: boolean;
  filename: string;
  totalImages: number;
  totalChunks: number;
  status: "preparing" | "ready" | "failed";
  createdAt: number;
}

interface ImageData {
  id: string;
  url: string;
  thumbnailUrl?: string;
  title: string;
  created_at: number;
  width: number;
  height: number;
  metadata?: Record<string, unknown>;
}

export const handler: Handlers = {
  async GET(_req, ctx: FreshContext) {
    const taskId = ctx.params.taskId;
    console.log(`[${taskId}] ğŸ“¥ å¼€å§‹ä¸‹è½½`);

    try {
      const kv = await getKv();
      
      // è·å–ä»»åŠ¡ä¿¡æ¯
      const taskResult = await kv.get<TaskMeta>(['tasks', taskId]);
      if (!taskResult.value) {
        return new Response('ä»»åŠ¡æœªæ‰¾åˆ°', { status: 404 });
      }
      
      const task = taskResult.value;
      console.log(`[${taskId}] ğŸ“Š æ‰¾åˆ°${task.totalImages}å¼ å›¾ç‰‡ï¼Œåˆ†å¸ƒåœ¨${task.totalChunks}ä¸ªæ•°æ®å—ä¸­`);

      // æ¸…ç†å¯èƒ½å­˜åœ¨çš„åƒµå°¸é”ï¼ˆé‡å¯åçš„é”éƒ½æ˜¯æ— æ•ˆçš„ï¼‰
      await cleanupZombieLocks(taskId, kv);

      // åˆ›å»ºæµå¼å“åº”
      const headers = new Headers({
        'Content-Type': 'application/zip',
        'Content-Disposition': `attachment; filename="${task.filename}"`,
        'Cache-Control': 'no-store, must-revalidate',
        'Accept-Ranges': 'none',
        'X-Content-Type-Options': 'nosniff',
        'Transfer-Encoding': 'chunked',
      });
      
      return new Response(
        new ReadableStream({
          async start(controller) {
            try {
              await processTaskSafely(controller, taskId, task, kv);
            } catch (error) {
              console.error(`[${taskId}] é”™è¯¯:`, error);
              
              // ç¡®ä¿æ¸…ç†é”
              await kv.delete(['task_lock', taskId]).catch(() => {});
              
              // Check if the stream is still writable before trying to send an error
              try {
                // Ensure we can still write to the controller
                if (controller.desiredSize !== null && controller.desiredSize >= 0) {
                  // If the error is about concurrent processing, send a special response
                  if (error.message && error.message.includes('ä»»åŠ¡æ­£åœ¨è¢«å¦ä¸€ä¸ªè¯·æ±‚å¤„ç†ä¸­')) {
                    const message = "ä¸‹è½½å¤„ç†ä¸­ï¼Œè¯·ç¨ç­‰ä¸€ä¼šå†ç‚¹å‡»ä¸‹è½½æŒ‰é’®...";
                    controller.enqueue(new TextEncoder().encode(message));
                    controller.close();
                  } else {
                    controller.error(error);
                  }
                } else {
                  // Stream is already closed or errored, just log it
                  console.log(`[${taskId}] Stream already closed, cannot send error`);
                }
              } catch (e) {
                console.error(`[${taskId}] æ§åˆ¶å™¨é”™è¯¯:`, e);
              }
            }
          },
          
          // Handle client disconnection/abort events
          cancel(reason) {
            console.log(`[${taskId}] ğŸš« Client disconnected: ${reason || 'Unknown reason'}`);
            
            // Clean up resources and release locks when client disconnects
            kv.delete(['task_lock', taskId]).catch(e => {
              console.error(`[${taskId}] Failed to release lock on disconnect:`, e);
            });
            
            // Store abort event in KV for tracking
            kv.set(['task_aborted', taskId], {
              timestamp: Date.now(),
              reason: String(reason || 'Client disconnected')
            }, { expireIn: 24 * 60 * 60 * 1000 }).catch(() => {});
          }
        }),
        { headers }
      );

    } catch (error) {
      console.error(`[${taskId}] è®¾ç½®é”™è¯¯:`, error);
      return new Response(`é”™è¯¯: ${error instanceof Error ? error.message : String(error)}`, { status: 500 });
    }
  },
};

/**
 * æ¸…ç†åƒµå°¸é”
 */
async function cleanupZombieLocks(taskId: string, kv: Deno.Kv): Promise<void> {
  try {
    const lockKey = ['task_lock', taskId];
    const existingLock = await kv.get(lockKey);
    
    if (existingLock.value) {
      const lockAge = Date.now() - ((existingLock.value as any).startTime || 0);
      // è¶…è¿‡2åˆ†é’Ÿçš„é”è®¤ä¸ºæ˜¯åƒµå°¸é”
      if (lockAge > 2 * 60 * 1000) {
        console.log(`[${taskId}] ğŸ§¹ æ¸…ç†åƒµå°¸é” (${Math.round(lockAge/1000)}ç§’å‰)`);
        await kv.delete(lockKey);
      }
    }
  } catch (error) {
    console.warn(`[${taskId}] æ¸…ç†åƒµå°¸é”å¤±è´¥:`, error);
  }
}

/**
 * å®‰å…¨çš„ä»»åŠ¡å¤„ç†
 */
async function processTaskSafely(
  controller: ReadableStreamDefaultController,
  taskId: string,
  task: TaskMeta,
  kv: Deno.Kv
) {
  let lockAcquired = false;
  let abortChecker: number | null = null;
  let isClientConnected = true;
  
  // Set up a mechanism to check if the client is still connected
  const setupAbortChecker = () => {
    // Store a client state object to be shared across the process
    const clientState = { disconnected: false, lastActivity: Date.now() };
    
    // Function to check connection every few seconds
    const checkConnection = async () => {
      try {
        // Check for aborted task flag in KV
        const aborted = await kv.get(['task_aborted', taskId]);
        if (aborted.value) {
          console.log(`[${taskId}] ğŸ›‘ Task was previously aborted, stopping processing`);
          clientState.disconnected = true;
          return;
        }
        
        // Check if we can still write to the controller
        if (!controller.desiredSize || controller.desiredSize < 0) {
          console.log(`[${taskId}] ğŸš« Client appears disconnected (controller closed)`);
          clientState.disconnected = true;
          
          // Clean up resources
          await kv.delete(['task_lock', taskId]).catch(() => {});
          
          // Record the abort event in KV
          await kv.set(['task_aborted', taskId], {
            timestamp: Date.now(),
            reason: 'Controller no longer writable'
          }, { expireIn: 24 * 60 * 60 * 1000 }).catch(() => {});
          
          return;
        }
        
        // If too much time has passed since last successful write, consider connection dead
        const timeSinceActivity = Date.now() - clientState.lastActivity;
        if (timeSinceActivity > 15000) { // 15 seconds of inactivity (reduced from 30)
          console.log(`[${taskId}] â±ï¸ No client activity for ${Math.round(timeSinceActivity/1000)}s`);
          clientState.disconnected = true;
          
          // Clean up resources
          await kv.delete(['task_lock', taskId]).catch(() => {});
          
          // Record the abort event in KV
          await kv.set(['task_aborted', taskId], {
            timestamp: Date.now(),
            reason: 'Client inactivity timeout'
          }, { expireIn: 24 * 60 * 60 * 1000 }).catch(() => {});
          
          return;
        }
        
        // Still connected, schedule next check
        if (!clientState.disconnected) {
          setTimeout(checkConnection, 3000); // Check every 3 seconds
        }
      } catch (e) {
        // Don't log every connection check error
        setTimeout(checkConnection, 1000);
      }
    };
    
    // Start checking for disconnection
    setTimeout(checkConnection, 3000);
    
    // Return the client state for the rest of the process to check
    return clientState;
  };
  
  // Initialize client state tracker
  const clientState = setupAbortChecker();
  
  try {
    // ğŸ”’ å°è¯•è·å–ä»»åŠ¡é”ï¼Œä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
    const lockKey = ['task_lock', taskId];
    const lockData = { startTime: Date.now(), pid: crypto.randomUUID() };
    
    const lockResult = await kv.atomic()
      .check({ key: lockKey, versionstamp: null })
      .set(lockKey, lockData, { expireIn: 5 * 60 * 1000 }) // 5åˆ†é’Ÿé”
      .commit();
      if (!lockResult.ok) {
      // æ£€æŸ¥é”çš„å¹´é¾„ï¼Œå¦‚æœå¤ªè€ç›´æ¥æŠ¢å 
      const existingLock = await kv.get(lockKey);
      if (existingLock.value) {
        const lockAge = Date.now() - ((existingLock.value as any).startTime || 0);
        
        // å¦‚æœé”è¿‡æœŸï¼ˆ2åˆ†é’Ÿï¼‰ï¼Œåˆ™å¼ºåˆ¶é‡Šæ”¾
        if (lockAge > 2 * 60 * 1000) { 
          console.warn(`[${taskId}] æŠ¢å è¿‡æœŸé” (${Math.round(lockAge/1000)}ç§’)`);
          await kv.delete(lockKey);
          
          const retryResult = await kv.atomic()
            .check({ key: lockKey, versionstamp: null })
            .set(lockKey, lockData, { expireIn: 5 * 60 * 1000 })
            .commit();
            
          if (!retryResult.ok) {
            throw new Error('æ— æ³•è·å–ä»»åŠ¡é”');
          }
          lockAcquired = true;
        } else {
          // å¦‚æœæ˜¯æœ€è¿‘çš„é”ï¼ˆ10ç§’å†…ï¼‰ï¼Œè¿”å›ä¸€ä¸ªç‰¹æ®Šå“åº”è€Œä¸æ˜¯é”™è¯¯
          // è¿™æ ·æµè§ˆå™¨æˆ–ä¸‹è½½ç®¡ç†å™¨ä¸ä¼šç«‹å³é‡è¯•ï¼Œç»™ä¹‹å‰çš„è¯·æ±‚ä¸€äº›æ—¶é—´å®Œæˆ
          if (lockAge < 10 * 1000) {
            console.log(`[${taskId}] â³ ä»»åŠ¡åˆšåˆšå¼€å§‹å¤„ç† (${Math.round(lockAge/1000)}ç§’å‰)ï¼Œè¿”å›é‡è¯•å“åº”`);
            controller.enqueue(new TextEncoder().encode("ZIP processing has just started. Please wait..."));
            controller.close();
            return; // Exit early without throwing an error
          }
          
          throw new Error('ä»»åŠ¡æ­£åœ¨è¢«å¦ä¸€ä¸ªè¯·æ±‚å¤„ç†ä¸­');
        }
      } else {
        throw new Error('æ— æ³•è·å–ä»»åŠ¡é”');
      }
    } else {
      lockAcquired = true;
    }
    
    console.log(`[${taskId}] ğŸ”’ è·å–ä»»åŠ¡é”`);
    
    let closed = false;
    
    // é…ç½®ä½å‹ç¼©ZIPä»¥å‡å°‘CPUä½¿ç”¨
    const zip = new fflate.Zip({
      level: 1,
      mem: 8
    });
    
    // ç«‹å³å‘é€ZIPæ•°æ®å—
    zip.ondata = (err, chunk, final) => {
      if (closed) return;
      
      if (err) {
        console.error(`[${taskId}] ZIPé”™è¯¯:`, err);
        if (!closed) {
          closed = true;
          controller.error(new Error(`ZIPé”™è¯¯: ${err.message}`));
        }
        return;
      }
      
      if (chunk && chunk.length > 0) {
        try {
          // Check for client disconnection before attempting to send data
          if (clientState.disconnected) {
            console.log(`[${taskId}] ğŸ“µ Client disconnected, stopping ZIP stream`);
            closed = true;
            return;
          }
          
          // Also check if the controller is still writable
          if (!controller.desiredSize || controller.desiredSize < 0) {
            console.log(`[${taskId}] âš ï¸ Stream no longer writable, marking as disconnected`);
            closed = true;
            clientState.disconnected = true;
            return;
          }
          
          // Only enqueue if we're sure the client is still connected
          controller.enqueue(chunk);
          // Update last activity timestamp when we successfully write to the stream
          clientState.lastActivity = Date.now();
        } catch (e) {
          console.error(`[${taskId}] æ§åˆ¶å™¨é”™è¯¯:`, e);
          closed = true;
          clientState.disconnected = true;
        }
      }
      
      if (final && !closed) {
        try {
          // One final check before closing
          if (!clientState.disconnected) {
            console.log(`[${taskId}] âœ… å®Œæˆ`);
            controller.close();
          }
        } catch (e) {
          console.error(`[${taskId}] å…³é—­é”™è¯¯:`, e);
        } finally {
          closed = true;
        }
      }
    };    // å…ˆå¤„ç†å…ƒæ•°æ®
    if (task.includeMetadata) {
      console.log(`[${taskId}] ğŸ“„ æ·»åŠ metadata.json`);
      
      // Check if client has disconnected before processing metadata
      if (clientState.disconnected) {
        console.log(`[${taskId}] ğŸ›‘ Skipping metadata due to client disconnection`);
      } else {
        await writeMetadataWithAbortCheck(zip, taskId, task, kv, clientState);
        
        if (!clientState.disconnected) {
          console.log(`[${taskId}] ğŸ§¹ ä»KVä¸­æ¸…é™¤å…ƒæ•°æ®`);
          await clearMetadata(taskId, task, kv);
        }
      }
    }// ç„¶åå¤„ç†å›¾ç‰‡
    console.log(`[${taskId}] ğŸ“¸ å¤„ç†å›¾ç‰‡ä¸­`);
    let successCount = 0;
    let errorCount = 0;
    
    // Modified to pass client state and check for disconnection
    await processImagesWithAbortCheck(zip, taskId, task, kv, clientState, (success) => {
      if (success) {
        successCount++;
        if (successCount % 20 === 0 || successCount + errorCount === task.totalImages) {
          console.log(`[${taskId}] ğŸ“Š è¿›åº¦: ${successCount + errorCount}/${task.totalImages} (${errorCount}ä¸ªé”™è¯¯)`);
        }
      } else {
        errorCount++;
      }
    });
    
    if (clientState.disconnected) {
      console.log(`[${taskId}] ğŸ›‘ Image processing aborted due to client disconnection`);
      // Don't finalize the ZIP since client is gone
      return;
    }
    
    console.log(`[${taskId}] ğŸ“Š æœ€ç»ˆç»“æœ: ${successCount + errorCount}/${task.totalImages} å®Œæˆ (${errorCount}ä¸ªé”™è¯¯)`);
    
    // å®ŒæˆZIP
    zip.end();
    
  } catch (error) {
    console.error(`[${taskId}] å¤„ç†é”™è¯¯:`, error);
    throw error;
  } finally {
    // ğŸ”’ é‡Šæ”¾ä»»åŠ¡é”
    if (lockAcquired) {
      try {
        await kv.delete(['task_lock', taskId]);
        console.log(`[${taskId}] ğŸ”“ é‡Šæ”¾ä»»åŠ¡é”`);
      } catch (lockError) {
        console.error(`[${taskId}] é‡Šæ”¾é”é”™è¯¯:`, lockError);
      }
    }
  }
}

/**
 * ä¿®å¤ç‰ˆçš„å›¾ç‰‡å¤„ç†å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬ï¼Œæ— éœ€å•ç‹¬è·Ÿè¸ªæ¯å¼ å›¾ç‰‡çš„è¿›åº¦
 */
async function processImagesFixed(
  zip: fflate.Zip, 
  taskId: string, 
  task: TaskMeta, 
  kv: Deno.Kv,
  progressCallback?: (success: boolean) => void
) {
  let processed = 0;
  
  for (let i = 0; i < task.totalChunks; i++) {
    console.log(`[${taskId}] ğŸ“¦ æ•°æ®å— ${i + 1}/${task.totalChunks}`);
    
    // å¼ºåˆ¶åƒåœ¾å›æ”¶
    try {
      // @ts-ignore
      if (globalThis.gc) globalThis.gc();
    } catch (e) {}
    
    // è·å–æ•°æ®å—
    const chunk = await kv.get<ImageData[]>(['img_chunks', taskId, i]);
    if (!chunk.value) continue;
    
    const batchSize = 3;
    const imageArray = [...chunk.value];
    chunk.value = null; // ç«‹å³æ¸…ç†å¼•ç”¨
    
    for (let j = 0; j < imageArray.length; j += batchSize) {
      const batchImages = imageArray.slice(j, j + batchSize);      for (const img of batchImages) {
        try {
          // å¤„ç†ä¸»å›¾
          await processImageWithRetry(img, zip, taskId, false);
          
        // å¤„ç†ç¼©ç•¥å›¾
          if (task.includeThumbnails && img.thumbnailUrl && img.thumbnailUrl !== img.url) {
            console.log(`[${taskId}] ğŸ–¼ï¸ Processing thumbnail for ${img.id}`);
            await processImageWithRetry(img, zip, taskId, true);
          }
          
          processed++;
          if (progressCallback) {
            progressCallback(true);
          }
          
        } catch (error) {
          console.error(`[${taskId}] âŒ å¤±è´¥ ${img.id}:`, error);
          
          if (progressCallback) {
            progressCallback(false);
          }
        }
      }
      
      batchImages.length = 0;
      await new Promise(resolve => setTimeout(resolve, 500));
      
      try {
        // @ts-ignore
        if (globalThis.gc) globalThis.gc();
      } catch (e) {}
    }
    
    imageArray.length = 0;
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
}

/**
 * å¸¦æœ‰ä¸­æ–­æ£€æŸ¥çš„å›¾ç‰‡å¤„ç†å‡½æ•°
 */
async function processImagesWithAbortCheck(
  zip: fflate.Zip, 
  taskId: string, 
  task: TaskMeta, 
  kv: Deno.Kv,
  clientState: { disconnected: boolean; lastActivity: number },
  progressCallback?: (success: boolean) => void
) {
  let processed = 0;
  let lastProgressLog = Date.now();
  let batchStart = Date.now();
  
  for (let i = 0; i < task.totalChunks; i++) {
    // Check if client has disconnected before processing each chunk
    if (clientState.disconnected) {
      console.log(`[${taskId}] ğŸ›‘ Aborting image processing due to client disconnection`);
      return;
    }
    
    console.log(`[${taskId}] ğŸ“¦ æ•°æ®å— ${i + 1}/${task.totalChunks}`);
    
    // å¼ºåˆ¶åƒåœ¾å›æ”¶
    try {
      // @ts-ignore
      if (globalThis.gc) globalThis.gc();
    } catch (e) {}
    
    // è·å–æ•°æ®å—
    const chunk = await kv.get<ImageData[]>(['img_chunks', taskId, i]);
    if (!chunk.value) continue;
    
    const batchSize = 3;
    const imageArray = [...chunk.value];
    chunk.value = null; // ç«‹å³æ¸…ç†å¼•ç”¨
    
    for (let j = 0; j < imageArray.length; j += batchSize) {
      // Check for disconnection before each batch
      if (clientState.disconnected) {
        console.log(`[${taskId}] ğŸ›‘ Aborting image batch due to client disconnection`);
        return;
      }
      
      const batchImages = imageArray.slice(j, j + batchSize);
      
      for (const img of batchImages) {
        try {
          // Check for disconnection before each image
          if (clientState.disconnected) {
            return;
          }
          
          // å¤„ç†ä¸»å›¾
          await processImageWithRetry(img, zip, taskId, false);
          
          // å¤„ç†ç¼©ç•¥å›¾ - only process if includeThumbnails is true AND the thumbnailUrl exists
          if (task.includeThumbnails && img.thumbnailUrl && img.thumbnailUrl !== img.url) {
            // Check for disconnection before processing thumbnail
            if (clientState.disconnected) {
              return;
            }
            
            // Reduce log verbosity - don't log every thumbnail processing
            await processImageWithRetry(img, zip, taskId, true);
          }
          
          processed++;
          if (progressCallback) {
            progressCallback(true);
          }
          
          // Only log progress periodically instead of for every image
          const now = Date.now();
          if (now - lastProgressLog > 5000) { // Only log every 5 seconds
            const processingRate = processed / ((now - batchStart) / 1000);
            console.log(`[${taskId}] ğŸ“Š è¿›åº¦: ${processed}/${task.totalImages} (${processingRate.toFixed(1)}å¼ /ç§’)`);
            lastProgressLog = now;
          }
          
        } catch (error) {
          console.error(`[${taskId}] âŒ å¤±è´¥ ${img.id.slice(-8)}:`, error);
          
          if (progressCallback) {
            progressCallback(false);
          }
        }
      }
      
      // Record progress in KV so it can be resumed if needed
      try {
        await kv.set(['task_progress', taskId], { 
          completedChunks: i + 1,
          totalProcessed: processed,
          lastUpdate: Date.now()
        }, { expireIn: 24 * 60 * 60 * 1000 });
      } catch (e) {
        console.warn(`[${taskId}] Failed to save progress:`, e);
      }
    }
    
    imageArray.length = 0;
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
}

/**
 * é‡è¯•å¤„ç†å›¾ç‰‡
 */
async function processImageWithRetry(img: ImageData, zip: fflate.Zip, taskId: string, isThumbnail: boolean, retries = 2) {
  // Get the appropriate URL based on whether we're processing a thumbnail or main image
  const url = isThumbnail ? img.thumbnailUrl : img.url;
  const imgId = img.id.slice(-8);
  
  // Skip invalid thumbnail URLs with a more thorough check
  if (isThumbnail) {
    if (!url || !url.startsWith('http')) {
      // Don't log every skipped thumbnail to reduce log spam
      return;
    }
  }
  
  let attempt = 0;
  let lastError: Error | null = null;
  
  while (attempt <= retries) {
    try {
      if (attempt > 0) {
        console.log(`[${taskId}] ğŸ”„ Retry ${attempt}/${retries} for ${imgId}`);
        await new Promise(resolve => setTimeout(resolve, 1000 * Math.pow(2, attempt - 1)));
      }
      
      await processImageStream(img, zip, taskId, isThumbnail);
      return;
      
    } catch (error) {
      lastError = error;
      console.error(`[${taskId}] âš ï¸ Attempt ${attempt + 1} failed for ${imgId}`);
      attempt++;
      
      try {
        // @ts-ignore
        if (globalThis.gc) globalThis.gc();
      } catch (e) {}
    }
  }
  
  if (isThumbnail) {
    // Don't log every failed thumbnail to reduce log spam
    return; // Don't throw error for thumbnails, just skip them
  }
  
  throw lastError || new Error("Failed to process image after retries");
}

/**
 * æµå¼å¤„ç†å›¾ç‰‡
 */
async function processImageStream(img: ImageData, zip: fflate.Zip, taskId: string, isThumbnail: boolean) {
  const url = isThumbnail ? img.thumbnailUrl! : img.url;
  const timeout = isThumbnail ? 15000 : 30000;
  const imgId = img.id.slice(-8); // Use shortened ID for logs to reduce verbosity
  
  // Use more concise logging to reduce verbosity
  console.log(`[${taskId}] ${isThumbnail ? 'ğŸ–¼ï¸' : 'ğŸŒ'} Fetching ${isThumbnail ? 'thumbnail' : 'image'} for ${imgId}`);
  
  // Skip invalid URLs
  if (!url || !url.startsWith('http')) {
    console.warn(`[${taskId}] âš ï¸ Invalid URL for ${isThumbnail ? 'thumbnail' : 'image'}: ${imgId}`);
    return; // Skip this image instead of throwing an error
  }
  
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeout);
  
  try {
    const response = await fetch(url, {
      signal: controller.signal,
      headers: { 'Accept': 'image/*' }
    });
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`);
    }
    
    const date = formatDateForFilename(img.created_at);
    const title = sanitizeFilename(img.title, 50);
    const id = imgId;
    const ext = getExtensionFromResponse(response, url);
    
    // Create folders inside the ZIP
    const folder = isThumbnail ? 'thumbnails' : 'images';
    const suffix = isThumbnail ? '_thumb' : '';
    const filename = `${folder}/${date}_${title}_${id}${suffix}.${ext}`;
    
    // More concise logging to avoid verbose output
    console.log(`[${taskId}] ğŸ“ Adding: ${filename}`);
    
    if (response.body) {
      const file = new fflate.ZipDeflate(filename, { level: 3 });
      
      try {
        // Add the file to zip only after we've successfully fetched it
        zip.add(file);
        
        const reader = response.body.getReader();
        const chunkSize = 64 * 1024;
        
        try {
          while (true) {
            const { done, value } = await reader.read();
            
            if (value && value.length > 0) {
              // Make sure we don't push any data if the stream is already in an error state
              try {
                file.push(value, done);
              } catch (pushError) {
                console.error(`[${taskId}] Error pushing data to ZIP:`, pushError);
                break;
              }
            } else if (done) {
              try {
                file.push(new Uint8Array(0), true);
              } catch (finalPushError) {
                console.error(`[${taskId}] Error finalizing ZIP entry:`, finalPushError);
              }
            }
            
            if (done) break;
            
            if (value && value.length >= chunkSize) {
              await new Promise(resolve => setTimeout(resolve, 10));
            }
          }
          
        } catch (streamError) {
          console.error(`[${taskId}] Stream processing error:`, streamError);
          throw streamError;
        } finally {
          try {
            reader.releaseLock();
          } catch (e) {}
        }
      } catch (zipError) {
        console.error(`[${taskId}] ZIP processing error:`, zipError);
        throw zipError;
      }
    } else {
      // For smaller responses that don't have a readable stream
      const arrayBuffer = await response.arrayBuffer();
      const data = new Uint8Array(arrayBuffer);
      
      try {
        const file = new fflate.ZipDeflate(filename, { level: 3 });
        zip.add(file);
        file.push(data, true);
      } catch (zipError) {
        console.error(`[${taskId}] ZIP processing error:`, zipError);
        throw zipError;
      }
    }
    
  } catch (error) {
    console.error(`[${taskId}] Processing error for ${imgId}:`, error);
    throw error;
  } finally {
    clearTimeout(timeoutId);
  }
}